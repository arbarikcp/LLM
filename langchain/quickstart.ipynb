{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (2.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_community in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (0.0.17)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (2.0.25)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (0.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (3.9.3)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (0.1.18)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_community) (0.0.86)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.20.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_community) (1.33)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_community) (23.2)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_community) (4.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_community) (3.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain_community) (2.4)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain_community) (2.16.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain_community) (2.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (1.7.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (0.1.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.86)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (0.1.18)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.17 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.17)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain) (4.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (3.6)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_openai in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (0.0.5)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_openai) (0.1.18)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_openai) (1.26.4)\n",
      "Requirement already satisfied: tiktoken<0.6.0,>=0.5.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_openai) (0.5.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain_openai) (1.11.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (0.0.86)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (2.6.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (8.2.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (23.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (2.31.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (1.33)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_openai) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain_openai) (2.4)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.26.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.1)\n",
      "Requirement already satisfied: certifi in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain_openai) (2.16.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain_openai) (2.2.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from tiktoken<0.6.0,>=0.5.2->langchain_openai) (2023.12.25)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip3 install langchain_community\n",
    "!pip3 install faiss-cpu\n",
    "!pip3 install langchain\n",
    "!pip3 install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "#load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith can help with testing in the following ways:\\n\\n1. Test Automation: Langsmith can generate test scripts and code snippets to automate the testing process. It can create test cases, stubs, and mocks for different scenarios, reducing the manual effort required for testing.\\n\\n2. Test Data Generation: Langsmith can generate synthetic test data for different test scenarios, enabling testers to cover a wide range of inputs and edge cases. This helps in identifying bugs and vulnerabilities that might not be evident with limited real-world data.\\n\\n3. Test Oracles: Langsmith can generate expected outputs or oracles for a given set of inputs. This can be useful in automatically comparing the actual output of a system with the expected output, allowing for quick identification of discrepancies.\\n\\n4. Test Coverage Analysis: Langsmith can analyze code and provide insights into the test coverage achieved by existing test cases. It can identify areas of the code that are not adequately covered by tests, enabling testers to focus their efforts on improving coverage.\\n\\n5. Test Prioritization: Langsmith can analyze the code and identify critical areas that require thorough testing. It can prioritize the order in which different test cases should be executed based on their impact on the system, helping testers maximize their testing efforts.\\n\\n6. Test Case Generation: Langsmith can automatically generate test cases based on predefined rules, constraints, or specifications. This can save time and effort for testers, especially when dealing with complex systems with a large number of test scenarios.\\n\\nOverall, Langsmith can enhance the testing process by automating repetitive tasks, generating test data, providing oracles, analyzing coverage, prioritizing tests, and generating test cases. This can improve the efficiency, effectiveness, and accuracy of the testing process.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "# resp = llm.invoke(\"how can langsmith help with testing?\")\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are world class technical documentation writer.'),\n",
       " HumanMessage(content='hello')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "prompt.format_prompt(input=\"hello\")\n",
    "prompt.format_messages(input=\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can be a valuable tool for testing in the following ways:\\n\\n1. Test Automation: Langsmith enables the creation and execution of automated tests for software applications. It provides a user-friendly interface for writing test scripts using a simple yet powerful language. Testers can define test cases, input data, expected results, and assertions, making it easier to automate the testing process.\\n\\n2. Test Data Generation: Langsmith has built-in capabilities for generating test data. Testers can use its data generation functions to create realistic and diverse datasets to use in their tests. This ensures comprehensive testing coverage and helps identify potential issues or edge cases that might not be apparent with limited or static test data.\\n\\n3. Test Orchestration: Langsmith allows testers to orchestrate and manage the execution of tests across different systems and environments. It provides features to define test suites, execute tests in parallel or sequentially, and generate detailed reports. Testers can easily configure and schedule test runs to ensure efficient test coverage and minimize manual effort.\\n\\n4. Test Case Management: Langsmith provides a mechanism for organizing and managing test cases. Testers can create test case repositories, define test case structures, and track execution status. This centralizes test case management and facilitates collaboration among testing teams, ensuring that all test cases are properly documented and executed.\\n\\n5. Integration Testing: Langsmith can be integrated with other testing tools and frameworks, such as unit testing frameworks or continuous integration systems. This allows testers to seamlessly integrate Langsmith scripts into their existing testing infrastructure, enabling comprehensive integration testing. By combining Langsmith with other tools, testers can achieve end-to-end testing coverage across various components and systems.\\n\\nOverall, Langsmith simplifies and streamlines the testing process, making it more efficient and effective. It enables testers to automate repetitive tasks, generate realistic test data, manage test cases, and orchestrate test execution, ultimately improving software quality and reducing time-to-market.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a chain by combining prompt and llm\n",
    "chain = prompt | llm\n",
    "\n",
    "#involke the chain\n",
    "resp = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "# This returns an AIMessage. We may need to parse it. we need its content property.\n",
    "# langchain also has inbuilt parsers\n",
    "resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can be a valuable tool in the testing process by assisting with automated testing, test case generation, and test result analysis. Here are some ways Langsmith can help with testing:\\n\\n1. Automated Testing: Langsmith can generate test scripts or code snippets in various programming languages to automate the testing process. It can provide specific code templates for different types of tests such as unit tests, integration tests, and end-to-end tests. These scripts can be easily integrated into testing frameworks like Selenium, JUnit, or PyTest to execute tests automatically.\\n\\n2. Test Case Generation: Langsmith can assist in generating test cases based on defined criteria or specifications. It can help identify edge cases, boundary values, and combinations of inputs that need to be tested. By leveraging natural language processing capabilities, Langsmith can extract requirements from documentation or user stories and generate test cases accordingly.\\n\\n3. Test Result Analysis: Langsmith can analyze test results and provide meaningful insights. It can automatically process test outputs, logs, and other test artifacts to identify patterns, anomalies, and potential issues. Langsmith can also generate reports or visualizations to help testers interpret the test results more effectively.\\n\\n4. Documentation: Langsmith can contribute to the creation of comprehensive testing documentation. It can generate test plans, test scripts, and test reports in a standardized format, ensuring consistency and clarity. This documentation can be shared with the testing team, stakeholders, or clients to provide a clear overview of the testing process and its outcomes.\\n\\n5. Error Analysis: Langsmith can help in identifying and analyzing errors or defects in the testing phase. By examining log files, error messages, and stack traces, it can suggest potential root causes and solutions for fixing the issues. This can save significant time and effort in troubleshooting and debugging.\\n\\nOverall, Langsmith can enhance the testing process by automating repetitive tasks, generating test cases, analyzing test results, providing documentation, and assisting in error analysis. It can improve the efficiency, accuracy, and effectiveness of testing activities, ultimately leading to higher software quality.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "# combine this output parser to pur chain\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# invoke the chain\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrival chain\n",
    "Lets read from langsmith document and provide the relevant text from the\n",
    "document to openAI for answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "number of docuemnts= 5\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "# print(docs)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(\"number of docuemnts=\",len(documents))\n",
    "vector = FAISS.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing by allowing users to visualize test results.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Running this document chain manually by passing some context text\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retrival chain by adding our vector store as a retiever. \n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Retrieval Chain\n",
    "\n",
    "we will create a new chain. This chain will take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "# test it bypassing a chat_history\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': \"LangSmith can help test your LLM applications by providing various features and functionalities. Here are a few ways LangSmith can assist in testing your LLM applications:\\n\\n1. Dataset Creation: LangSmith simplifies dataset creation by allowing you to curate datasets easily. These datasets can be used to test changes to prompts or chains in your LLM application.\\n\\n2. Visualizing Outputs: You can run your LLM chains over data points and visualize the outputs. LangSmith's client makes it easy to pull down a dataset and run a chain over them, logging the results to a new project associated with the dataset. You can review the outputs and assign feedback to runs, marking them as correct or incorrect.\\n\\n3. Evaluators: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While not perfect, these evaluators can guide your attention to examples that require manual review.\\n\\n4. Human Evaluation: LangSmith offers annotation queues that allow you to manually review and annotate runs. You can select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. As a reviewer, you can quickly step through the runs, view the input, output, and existing tags, and add your own feedback. This is particularly useful for assessing subjective qualities like creativity or humor and validating auto-evaluated runs.\\n\\nBy utilizing these features, LangSmith can help you thoroughly test and evaluate your LLM applications, ensuring their quality and reliability.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9899.72s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain-community in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (0.0.17)\n",
      "Requirement already satisfied: tavily-python in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (0.3.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.6.4)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.1.18)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.0.86)\n",
      "Requirement already satisfied: tiktoken==0.5.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from tavily-python) (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from tiktoken==0.5.2->tavily-python) (2023.12.25)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-community) (1.33)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2,>=0.1.16->langchain-community) (23.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-community) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain-community) (2.4)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain-community) (2.16.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain-community) (2.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9906.86s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2\n",
      "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchainhub) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bhakti/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchainhub) (3.6)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.20240125\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -U langchain-community tavily-python\n",
    "! pip3 install langchainhub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help with testing in several ways:\n",
      "\n",
      "1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different execution paths and edge cases.\n",
      "\n",
      "2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the specifications and requirements of the software to automatically generate assertions or checks that can be used as test oracles.\n",
      "\n",
      "3. Test Coverage Analysis: LangSmith can analyze the code coverage of the test cases to determine which parts of the software have been exercised during testing. This helps in identifying areas of the code that have not been adequately tested and may require additional test cases.\n",
      "\n",
      "4. Test Prioritization: LangSmith can prioritize the execution of test cases based on their likelihood of finding bugs or their impact on the system. It uses techniques such as mutation analysis and fault localization to identify the most critical test cases that should be executed first.\n",
      "\n",
      "5. Test Result Analysis: LangSmith can analyze the results of the test cases to identify patterns or trends that may indicate the presence of bugs or performance issues. It can also help in debugging and diagnosing the root cause of failures.\n",
      "\n",
      "Overall, LangSmith provides automated and intelligent testing capabilities that can improve the efficiency and effectiveness of the testing process.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'weather in Hyderabad'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://www.whereandwhen.net/when/central-and-south-asia/india/hyderabad/february/', 'content': 'Best time to go to Hyderabad? Weather in Hyderabad in february 2024  Seasonal average climate and temperature of Hyderabad in february  How was the weather last february? Here is the day by day recorded weather in Hyderabad in february 2023:  The climate of Hyderabad in february is perfectWeather in Hyderabad in february 2024. The weather in Hyderabad in the month of february comes from statistical datas on the last years. You can view the weather statistics for all the month, but also by using the tabs for the beginning, the middle and the end of the month. ... 06-02-2023 67°F to 97°F. 07-02-2023 67°F to 99°F. 08-02-2023 68 ...'}, {'url': 'https://weatherspark.com/h/y/109450/2024/Historical-Weather-during-2024-in-Hyderabad-India', 'content': '2024 Weather History in Hyderabad India  Cloud Cover in 2024 in Hyderabad Observed Weather in 2024 in Hyderabad  Hyderabad Temperature History 2024 Hourly Temperature in 2024 in Hyderabad Compare Hyderabad to another city:  Hours of Daylight and Twilight in 2024 in Hyderabad Sunrise & Sunset with Twilight in 2024 in Hyderabad79 °F warm Precipitation No Report Wind 6.9 mph light breeze Wind Dir. 100 deg, E Cloud Cover Mostly Clear 1,500 ft Raw: VOHY 280600Z 10006KT 4000 HZ FEW015 26/12 Q1022 NOSIG This report shows the past weather for Hyderabad, providing a weather history for 2024.'}, {'url': 'https://en.climate-data.org/asia/india/hyderabad/hyderabad-2801/t/february-2/', 'content': 'Hyderabad Weather in February  all information about the weather in Hyderabad in February:  Hyderabad weather in February Hyderabad weather by month // weather averages 22.2 (72) 16 (60.8) 28.6 (83.4) 7 (0.3)  Hyderabad weather in February // weather averages Airport close to HyderabadJanuary February March April May June July August September October November December; Avg. Temperature °C (°F) 22.2 °C (72) °F. 24.9 °C (76.9) °F. 28.2 °C'}, {'url': 'https://www.weather25.com/asia/india/andhra-pradesh/hyderabad', 'content': 'will give you an idea of weather trends in Hyderabad. For example, the weather in Hyderabad in February 2024.  The weather today in Hyderabad Hyderabad weather report  The weather in Hyderabad, India Hyderabad weather by months Hyderabad weather Hyderabad 14 day weather  Weather25.com provides all the information that you need to know about the weather in Hyderabad, India. Stay up to date01 January 02 February 03 March 04 April 05 May 06 June 07 July 08 August 09 September 10 October 11 November 12 December. ... the weather in Hyderabad in January 2024. These trends can be helpful when planning trips to Hyderabad or preparing for the weather in advance. There are many factors to consider when looking at the weather in Hyderabad ...'}, {'url': 'https://telugu.abplive.com/news/weather-in-telangana-andhrapradesh-hyderabad-on-6-february-2024-winter-updates-latest-news-here-143316', 'content': 'Hyderabad Weather: హైదరాబాద్ వాతావరణంహైదరాబాద్ లో ఆకాశం పాక్షికంగా మేఘాలు పట్టి ఉంటుంది. ఉదయం వేళల్లో పొగమంచు  Andhra Pradesh Weather: ప్రస్తుతం ఆంధ్రప్రదేశ్, యానాంలో దిగువ ట్రోపో ఆవరణలో ఆగ్నేయ, నైరుతి దిశల్లో గాలులు వీస్తున్నాయని  Weather Latest News: ఈ రోజు కింది స్థాయిలోని గాలులు దక్షిణ, ఆగ్నేయ దిశల నుంచి తెలంగాణ రాష్ట్రం వైపుకి వీస్తున్నాయని  Weather Latest Update: కనిష్ఠ ఉష్ణోగ్రతలు క్రమంగా పైకి - పగటి పూట వేడి కూడా: ఐఎండీWeather in Telangana Andhrapradesh Hyderabad on 6 February 2024 Winter updates latest news here | Weather Latest Update: కనిష్ఠ ఉష్ణోగ్రతలు క్రమంగా పైకి - పగటి పూట వేడి కూడా: ఐఎండీ Advertisement హోమ్ న్యూస్ Weather Latest Update: కనిష్ఠ ఉష్ణోగ్రతలు క్రమంగా పైకి - పగటి పూట వేడి కూడా: ఐఎండీ'}]\u001b[0m\u001b[32;1m\u001b[1;3mI found some information about the weather in Hyderabad. Here are a few sources you can check for more details:\n",
      "\n",
      "1. [WhereAndWhen.net](https://www.whereandwhen.net/when/central-and-south-asia/india/hyderabad/february/): This website provides seasonal average climate and temperature information for Hyderabad in February.\n",
      "\n",
      "2. [WeatherSpark](https://weatherspark.com/h/y/109450/2024/Historical-Weather-during-2024-in-Hyderabad-India): This website offers historical weather data for Hyderabad in 2024, including temperature, cloud cover, and wind information.\n",
      "\n",
      "3. [Climate-Data.org](https://en.climate-data.org/asia/india/hyderabad/hyderabad-2801/t/february-2/): This website provides weather averages for Hyderabad in February, including average temperature.\n",
      "\n",
      "4. [Weather25.com](https://www.weather25.com/asia/india/andhra-pradesh/hyderabad): This website offers 14-day weather forecasts for Hyderabad and provides information about weather trends.\n",
      "\n",
      "Please visit these sources for more specific information about the current weather in Hyderabad.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the weather in Hyderabad?',\n",
       " 'output': 'I found some information about the weather in Hyderabad. Here are a few sources you can check for more details:\\n\\n1. [WhereAndWhen.net](https://www.whereandwhen.net/when/central-and-south-asia/india/hyderabad/february/): This website provides seasonal average climate and temperature information for Hyderabad in February.\\n\\n2. [WeatherSpark](https://weatherspark.com/h/y/109450/2024/Historical-Weather-during-2024-in-Hyderabad-India): This website offers historical weather data for Hyderabad in 2024, including temperature, cloud cover, and wind information.\\n\\n3. [Climate-Data.org](https://en.climate-data.org/asia/india/hyderabad/hyderabad-2801/t/february-2/): This website provides weather averages for Hyderabad in February, including average temperature.\\n\\n4. [Weather25.com](https://www.weather25.com/asia/india/andhra-pradesh/hyderabad): This website offers 14-day weather forecasts for Hyderabad and provides information about weather trends.\\n\\nPlease visit these sources for more specific information about the current weather in Hyderabad.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")\n",
    "\n",
    "search = TavilySearchResults()\n",
    "\n",
    "tools = [retriever_tool, search]\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "agent_executor.invoke({\"input\": \"what is the weather in Hyderabad?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
